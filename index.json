[{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/posts/","section":"","summary":"","title":"","type":"posts"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/tags/docker/","section":"Tags","summary":"","title":"Docker","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/","section":"Learn Something New","summary":"","title":"Learn Something New","type":"page"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/tags/ntfy/","section":"Tags","summary":"","title":"Ntfy","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/tags/pihole/","section":"Tags","summary":"","title":"PiHole","type":"tags"},{"content":"TL;DR If you want to skip straight to the goods, the GitHub repository from the previous article has been updated with the full client-server configuration, PiHole and Traefik setup: here.\nIf you\u0026rsquo;ve read the previous article, you\u0026rsquo;ll know that I promised a follow-up covering the evolution of my WireGuard Docker stack. What started as a single VPN client container with a kill switch has since grown into something more ambitious: a single WireGuard container acting as both a Mullvad VPN client and a WireGuard server, with PiHole handling DNS for every device on the network and Traefik managing HTTPS reverse proxying for all self-hosted services. That\u0026rsquo;s a lot happening in one container\u0026rsquo;s network namespace, but it works well and is straightforward to configure once you understand how the pieces fit together.\nThe goal is straightforward: connect my devices to my home server from anywhere in the world, access all self-hosted services over HTTPS with valid certificates, have PiHole block ads on every connected device, and have all outbound traffic exit through Mullvad VPN. If the VPN connection drops, nothing gets through. Fail closed, as we discussed in the previous article.\nThe Architecture # Before diving into configuration files, let me outline what we\u0026rsquo;re building. The WireGuard container sits at the centre of everything. It runs two WireGuard interfaces simultaneously: wg0 acts as the server, accepting connections from remote clients like my laptop and phone, while wg1 acts as the VPN client, tunnelling outbound traffic through Mullvad. PiHole and Traefik both run on the WireGuard container\u0026rsquo;s network stack using Docker\u0026rsquo;s network_mode: service:wireguard1, meaning they share its network interfaces — including both WireGuard tunnels.\nWhen a remote client connects via WireGuard, their traffic enters through wg0, gets forwarded through the container, and exits via wg1 to Mullvad. DNS queries go to PiHole (running on the tunnel IP), which blocks ads and resolves *.jmartins.dev to the tunnel address where Traefik is listening. Traefik then routes the request to the correct service based on the hostname. All of this happens without any service being directly exposed to the internet.\nTwo Interfaces, One Container # In the previous article, our WireGuard container ran a single interface (wg0) as a Mullvad VPN client. Now we need two: a server interface for accepting remote connections and a client interface for the outbound VPN tunnel. The Linuxserver.io WireGuard image makes this relatively painless.\nThe Server Interface # Setting the PEERS environment variable on the Linuxserver.io WireGuard container triggers its server mode2. The image generates a server configuration and individual peer configurations that can be imported on your devices.\n### DOCKER-COMPOSE.YAML — WIREGUARD SERVICE (PARTIAL) ### services: wireguard: image: lscr.io/linuxserver/wireguard:latest container_name: wireguard hostname: wireguard cap_add: - NET_ADMIN environment: - PUID=${PUID} - PGID=${PGID} - TZ=${TZ} - PEERS=laptop,phone - SERVERURL=wireguard.example.com - SERVERPORT=${WIREGUARD_PORT} - INTERNAL_SUBNET=10.0.2.0/24 - PEERDNS=10.0.2.1 - ALLOWEDIPS=0.0.0.0/0 - LOG_CONFS=false volumes: - ${CONFIG_DIR}/wireguard:/config - ${CONFIG_DIR}/wireguard_startup:/custom-cont-init.d:ro ports: - 80:80 - 443:443 - ${WIREGUARD_PORT}:${WIREGUARD_PORT}/udp sysctls: - net.ipv4.conf.all.src_valid_mark=1 healthcheck: test: ping -c 1 1.1.1.1 || exit 1 interval: 2s start_period: 10s start_interval: 2s # requires Docker Compose v2.20+ / Docker Engine 25+ timeout: 5s retries: 3 restart: always A few things stand out compared to the previous article. We\u0026rsquo;re now exposing the WireGuard port (UDP only — WireGuard doesn\u0026rsquo;t use TCP) for incoming peer connections, as well as ports 80 and 443 for the web services we\u0026rsquo;ll be routing through Traefik later. The PEERDNS variable tells the generated peer configurations to use 10.0.2.1 as their DNS server — this will be PiHole, running on the WireGuard container\u0026rsquo;s network stack at the server\u0026rsquo;s tunnel address. ALLOWEDIPS=0.0.0.0/0 means the generated peer configs will route all client traffic through the tunnel, not just traffic destined for the server\u0026rsquo;s subnet. LOG_CONFS=false prevents the image from logging the generated configurations to the container output, which you probably want when your private keys are involved.\nOn first run, the image generates a server configuration in /config/wg_confs/wg0.conf along with peer configurations in /config/peer_laptop/, /config/peer_phone/, and so on. We need to modify the generated server config to add forwarding and NAT rules that will route peer traffic through the Mullvad tunnel:\n[Interface] Address = 10.0.2.1 ListenPort = 51820 PrivateKey = [REDACTED] PostUp = iptables -A FORWARD -i %i -j ACCEPT; iptables -A FORWARD -o %i -j ACCEPT; iptables -t nat -A POSTROUTING -o wg1 -j MASQUERADE FwMark = 51820 [Peer] # peer_laptop PublicKey = [REDACTED] AllowedIPs = 10.0.2.2/32 [Peer] # peer_phone PublicKey = [REDACTED] AllowedIPs = 10.0.2.3/32 The PostUp directive is the key to the dual-interface setup. When the server interface comes up, it:\nAllows packet forwarding in both directions through wg0, so traffic from peers can be routed through the container. Masquerades all traffic leaving through wg1 (the Mullvad tunnel), so peers\u0026rsquo; outbound internet traffic appears to originate from the container\u0026rsquo;s Mullvad VPN address. The FwMark = 51820 is critical — it marks the server\u0026rsquo;s own encrypted UDP packets with this value. As we\u0026rsquo;ll see when we update the kill switch, these marked packets are exempt from the outbound blocking rules, allowing the server to communicate with its remote peers regardless of the state of the Mullvad connection.\nYou\u0026rsquo;ll notice there\u0026rsquo;s no corresponding PostDown directive to clean up these iptables rules. In a Docker context this is fine — restarting the interface means restarting the entire container, which resets iptables state. If you adapt this setup for a non-containerised host, you\u0026rsquo;ll want to add PostDown rules that mirror the PostUp with -D (delete) instead of -A (append) to avoid accumulating duplicate rules on interface restarts.\nThe Client Interface # The Mullvad VPN client configuration from the previous article is now wg1.conf instead of wg0.conf, placed directly in the /config directory. The configuration is largely the same, with one important change:\n[Interface] PrivateKey = [REDACTED] Address = 10.64.114.74/32 DNS = 127.0.0.1 [Peer] PublicKey = [REDACTED] AllowedIPs = 0.0.0.0/0 Endpoint = 89.44.10.178:51820 The DNS field now points to 127.0.0.1 instead of Mullvad\u0026rsquo;s DNS server. Since PiHole runs on the WireGuard container\u0026rsquo;s network stack, localhost resolves to PiHole. This means even the container\u0026rsquo;s own DNS queries go through PiHole, getting the benefit of ad blocking and our custom domain resolution.\nThere\u0026rsquo;s a subtlety here: wg-quick up sets 127.0.0.1 as the system resolver when it brings up wg1, but PiHole hasn\u0026rsquo;t started yet at this point (it depends on the WireGuard container being healthy first). This creates a brief window where DNS is unavailable inside the container. In practice this doesn\u0026rsquo;t matter — the healthcheck uses an IP address (ping -c 1 1.1.1.1), and the dependency chain ensures no service that needs DNS starts until PiHole is up. Just be aware of this if you modify the startup order.\nWe\u0026rsquo;ve also removed the PostUp kill switch directive from the client config. As we discussed in the previous article, relying on the WireGuard configuration file for the kill switch means a parsing error could leave us in a fail open state. We\u0026rsquo;ll continue handling the kill switch in the startup script instead.\nThe Updated Kill Switch # With two WireGuard interfaces, the startup script from the previous article needs updating. The kill switch now targets wg1 (the Mullvad tunnel) instead of wg0, and we need to handle the client connection startup ourselves since the Linuxserver.io image only manages the server interface automatically.\n#!/bin/bash set -e echo \u0026#34;**** Adding iptables rules ****\u0026#34; DROUTE=$(ip route | grep default | awk \u0026#39;{print $3}\u0026#39;) HOMENET=192.168.0.0/16 HOMENET2=10.0.0.0/8 HOMENET3=172.16.0.0/12 ip route add $HOMENET3 via $DROUTE ip route add $HOMENET2 via $DROUTE ip route add $HOMENET via $DROUTE iptables -I OUTPUT -d $HOMENET -j ACCEPT iptables -A OUTPUT -d $HOMENET2 -j ACCEPT iptables -A OUTPUT -d $HOMENET3 -j ACCEPT # Kill switch iptables -A OUTPUT ! -o wg1 -m mark ! --mark 0xca6c -m addrtype ! --dst-type LOCAL -j REJECT wg-quick up /config/wg1.conf echo \u0026#34;**** Successfully added iptables rules ****\u0026#34; This script runs during container initialisation, before either WireGuard interface is brought up3. Compared to the script from the previous article, there are three important changes:\nLocal network routes: We add explicit routes for RFC 19184 private address ranges via the container\u0026rsquo;s default gateway. Without these, traffic destined for the Docker networks and your home network would attempt to route through the VPN tunnel. The corresponding iptables rules ensure that outbound traffic to these subnets is always allowed, regardless of the kill switch state.\nKill switch targeting wg1: The kill switch rule now references wg1 instead of wg0. It rejects all outbound traffic that is not going through the Mullvad tunnel, not marked with 0xca6c (the hexadecimal representation of port 51820), and not destined for a local address. The FwMark = 51820 we set on the server interface ensures its encrypted packets to remote peers are marked and thus exempted — without this, the server wouldn\u0026rsquo;t be able to communicate with its clients.\nManual client startup: We bring up wg1 ourselves with wg-quick up. The Linuxserver.io image handles wg0 (the server), but since wg1 is our custom addition, we need to start it explicitly.\nThe execution order is then:\nContainer starts, startup script runs. Kill switch and local network rules are set — outbound internet traffic is now blocked. wg1 (Mullvad client) is brought up — outbound traffic through the VPN is now allowed. Linuxserver.io image brings up wg0 (server) — remote peers can now connect. If the startup script fails or the Mullvad connection cannot be established, the kill switch is already in place. Fail closed.\nPiHole: DNS for the Stack and Beyond # PiHole5 needs no introduction to the self-hosting crowd, but its role in this stack goes beyond blocking ads. By running PiHole on the WireGuard container\u0026rsquo;s network stack, it becomes the DNS server for both local containers and remote WireGuard peers.\n### DOCKER-COMPOSE.YAML — PIHOLE SERVICE ### pihole: container_name: pihole image: pihole/pihole:latest network_mode: service:wireguard depends_on: wireguard: condition: service_healthy healthcheck: test: ping -c 1 google.com || exit 1 interval: 2s start_period: 10s start_interval: 2s timeout: 5s retries: 3 environment: TZ: ${TZ} FTLCONF_webserver_port: ${PIHOLE_WEBUI_PORT} FTLCONF_webserver_api_password: ${PIHOLE_PASSWORD} FTLCONF_dns_upstreams: \u0026#39;1.1.1.1\u0026#39; FTLCONF_dns_dnssec: true FTLCONF_dns_revServers: \u0026#39;true,192.168.1.0/24,192.168.1.1,lan\u0026#39; FTLCONF_misc_dnsmasq_lines: \u0026#34;address=/jmartins.dev/10.0.2.1\u0026#34; cap_add: - NET_ADMIN volumes: - ${CONFIG_DIR}/pihole/etc-pihole/:/etc/pihole/ - ${CONFIG_DIR}/pihole/etc-dnsmasq.d/:/etc/dnsmasq.d/ restart: always The network_mode: service:wireguard directive means PiHole shares the WireGuard container\u0026rsquo;s entire network namespace — all its interfaces, IP addresses, and routing tables. Since the WireGuard server\u0026rsquo;s tunnel address is 10.0.2.1, PiHole is reachable at 10.0.2.1:53 from any connected peer.\nThe important line here is:\nFTLCONF_misc_dnsmasq_lines: \u0026#34;address=/jmartins.dev/10.0.2.1\u0026#34; This dnsmasq directive tells PiHole to resolve any subdomain of jmartins.dev to 10.0.2.1. So when a remote client\u0026rsquo;s browser requests jellyfin.jmartins.dev, PiHole responds with 10.0.2.1 — the WireGuard tunnel address where Traefik is listening. The request stays inside the tunnel the entire way.\nUpstream DNS is set to 1.1.1.1 (Cloudflare), with DNSSEC enabled for good measure. Since all outbound traffic from the container exits through the Mullvad VPN, even these upstream DNS queries are encrypted and anonymised.\nRemember that we set PEERDNS=10.0.2.1 in the WireGuard container\u0026rsquo;s environment. This means the auto-generated peer configurations include DNS = 10.0.2.1, so every device that connects via WireGuard automatically uses PiHole. No client-side configuration needed — your phone gets ad blocking the moment it connects to the VPN.\nNote on PiHole healthcheck # PiHole\u0026rsquo;s healthcheck pings google.com rather than an IP address. This is intentional — it validates that both the VPN connection and DNS resolution are working. Services that depend on PiHole (depends_on: pihole: condition: service_healthy) won\u0026rsquo;t start until DNS is fully operational, preventing a cascade of failures from containers that can\u0026rsquo;t resolve hostnames.\nTraefik: The Reverse Proxy # With DNS sorted, we need something to actually handle the HTTPS requests arriving at 10.0.2.1. Enter Traefik6, a reverse proxy that can automatically discover Docker services and route traffic based on labels.\nDocker Socket Proxy # Before configuring Traefik, a brief detour on security. Traefik\u0026rsquo;s Docker provider needs access to the Docker socket to discover services, but mounting /var/run/docker.sock directly into a container exposes the full Docker API, which effectively grants root-equivalent access to the host. If Traefik were ever compromised, the attacker would have unrestricted control over every container and volume on the machine.\nInstead, we use a Docker Socket Proxy7 that exposes only the specific Docker API endpoints Traefik needs:\n### DOCKER-COMPOSE.YAML — DOCKER SOCKET PROXY ### proxy: image: tecnativa/docker-socket-proxy container_name: proxy environment: - CONTAINERS=1 - SERVICES=1 - NETWORKS=1 - TASKS=1 - IMAGES=1 volumes: - /var/run/docker.sock:/var/run/docker.sock:ro networks: - internal restart: always The proxy runs on a dedicated internal Docker network marked with internal: true, meaning containers on this network cannot access the internet. Only containers explicitly connected to the internal network can reach the proxy. We\u0026rsquo;ll need the WireGuard container connected to both the default and internal networks so that Traefik (running on its network stack) can reach the proxy by its container hostname.\nThe Traefik Service # ### DOCKER-COMPOSE.YAML — TRAEFIK SERVICE ### traefik: image: traefik container_name: traefik network_mode: service:wireguard volumes: - /etc/localtime:/etc/localtime:ro - ${CONFIG_DIR}/traefik/letsencrypt:/letsencrypt command: - --api.dashboard=true # LetsEncrypt with Cloudflare DNS challenge - --certificatesresolvers.letsencrypt.acme.dnschallenge=true - --certificatesresolvers.letsencrypt.acme.dnschallenge.provider=cloudflare - --certificatesresolvers.letsencrypt.acme.email=me@example.com - --certificatesresolvers.letsencrypt.acme.storage=/letsencrypt/acme.json # Entrypoints - --entrypoints.web.address=:80 - --entrypoints.websecure.address=:443 - --entrypoints.web.http.redirections.entryPoint.to=websecure - --entrypoints.web.http.redirections.entryPoint.scheme=https # Docker provider via socket proxy - --providers.docker=true - --providers.docker.exposedbydefault=false - --providers.docker.endpoint=tcp://proxy:2375 # Wildcard TLS - --entrypoints.websecure.http.tls=true - --entrypoints.websecure.http.tls.certResolver=letsencrypt - --entrypoints.websecure.http.tls.domains[0].main=jmartins.dev - --entrypoints.websecure.http.tls.domains[0].sans=*.jmartins.dev - --log.level=INFO environment: - CF_DNS_API_TOKEN=[REDACTED] depends_on: pihole: condition: service_healthy proxy: condition: service_started restart: always A few details worth calling out:\nDNS challenge for Let\u0026rsquo;s Encrypt: Since our services are only accessible through the WireGuard tunnel, the standard HTTP-01 challenge won\u0026rsquo;t work — Let\u0026rsquo;s Encrypt can\u0026rsquo;t reach our server over the public internet to validate ownership. We use the DNS-01 challenge8 with Cloudflare as the DNS provider instead. Traefik automatically creates the necessary DNS TXT records to prove domain ownership and obtains a wildcard certificate for *.jmartins.dev.\nWildcard certificate: Rather than requesting individual certificates for each subdomain, a single wildcard certificate covers the lot. Adding a new service is as simple as adding a Traefik label with the right Host() rule — no new certificate needed, no rate limit concerns.\nHTTP to HTTPS redirect: The web entrypoint on port 80 automatically redirects all traffic to websecure on port 443.\nService Routing with Labels # Since Traefik uses network_mode: service:wireguard, it shares the WireGuard container\u0026rsquo;s network namespace. This means that any service also sharing that network namespace (via network_mode: service:wireguard) is reachable from Traefik at localhost:\u0026lt;port\u0026gt;. However, Traefik discovers services through Docker labels, and since these co-located services don\u0026rsquo;t have their own network identity from Docker\u0026rsquo;s perspective, their routing labels need to go on the WireGuard container:\n### LABELS ON THE WIREGUARD CONTAINER ### labels: - traefik.enable=true ## PiHole - traefik.http.routers.pihole.entrypoints=websecure - traefik.http.routers.pihole.rule=Host(`pihole.jmartins.dev`) - traefik.http.routers.pihole.service=pihole - traefik.http.services.pihole.loadbalancer.server.scheme=http - traefik.http.services.pihole.loadbalancer.server.port=${PIHOLE_WEBUI_PORT} ## Jellyfin - traefik.http.routers.jellyfin.entrypoints=websecure - traefik.http.routers.jellyfin.rule=Host(`jellyfin.jmartins.dev`) - traefik.http.routers.jellyfin.tls.certresolver=letsencrypt - traefik.http.routers.jellyfin.service=jellyfin - traefik.http.services.jellyfin.loadbalancer.server.scheme=http - traefik.http.services.jellyfin.loadbalancer.server.port=${JELLYFIN_WEBUI_PORT} Each block defines a router (matching on hostname) and a service (pointing to the correct port). The pattern repeats for every service you want to expose — add a Host() rule, point it at the right port, and Traefik handles the rest. Services that have their own Docker network (not using network_mode: service:wireguard) can define labels directly on their own container definitions instead.\nServices Outside the Kill Switch # Not every service belongs behind the kill switch. Consider a notification service like ntfy9 — its entire purpose is to send push notifications to your devices. If the VPN connection drops and the kill switch activates, a notification service sharing the WireGuard network would be blocked from reaching the internet along with everything else. That\u0026rsquo;s precisely the moment you want a notification telling you that egress has stopped.\nThe solution is to give the service its own Docker network identity instead of sharing WireGuard\u0026rsquo;s. Since it\u0026rsquo;s not using network_mode: service:wireguard, it has its own outbound route that bypasses the kill switch entirely. Traefik can still route to it — the Docker provider discovers it by its own labels, and the WireGuard container\u0026rsquo;s connection to the default Docker network means Traefik can reach it over that network.\n### DOCKER-COMPOSE.YAML — NTFY SERVICE ### ntfy: image: binwiederhier/ntfy container_name: ntfy command: - serve environment: - TZ=${TZ} - NTFY_BASE_URL=https://ntfy.jmartins.dev - NTFY_AUTH_DEFAULT_ACCESS=deny-all - NTFY_BEHIND_PROXY=true - NTFY_ENABLE_LOGIN=true user: ${PUID}:${PGID} volumes: - ${CONFIG_DIR}/ntfy_cache:/var/lib/ntfy - ${CONFIG_DIR}/ntfy_config:/etc/ntfy healthcheck: test: [\u0026#34;CMD-SHELL\u0026#34;, \u0026#34;wget -q --tries=1 http://localhost:80/v1/health -O - | grep -Eo \u0026#39;\\\u0026#34;healthy\\\u0026#34;\\\\s*:\\\\s*true\u0026#39; || exit 1\u0026#34;] interval: 60s timeout: 10s retries: 3 start_period: 40s labels: - traefik.enable=true - traefik.http.routers.ntfy.entrypoints=websecure - traefik.http.routers.ntfy.rule=Host(`ntfy.jmartins.dev`) - traefik.http.routers.ntfy.tls.certresolver=letsencrypt - traefik.http.routers.ntfy.service=ntfy - traefik.http.services.ntfy.loadbalancer.server.scheme=http - traefik.http.services.ntfy.loadbalancer.server.port=80 restart: always Notice the difference: the Traefik labels are on the ntfy container itself rather than on the wireguard container. From a remote client\u0026rsquo;s perspective the experience is identical — ntfy.jmartins.dev resolves to 10.0.2.1 via PiHole, Traefik routes it to the ntfy container over the Docker network, and the response travels back through the tunnel. The only difference is what happens to ntfy\u0026rsquo;s outbound traffic: it goes directly through the host\u0026rsquo;s network rather than through Mullvad, so it can still deliver notifications when the VPN is down.\nThis pattern applies to any service where continued outbound connectivity is more important than routing through the VPN — monitoring, alerting, and dynamic DNS updaters are common examples.\nPutting It All Together # Now we\u0026rsquo;re ready to build the full stack. Here\u0026rsquo;s the complete docker-compose.yaml with the WireGuard client-server, PiHole, Traefik, the Docker Socket Proxy, Jellyfin as an example service behind the kill switch, and ntfy as an example service outside it:\n### DOCKER-COMPOSE.YAML ### services: wireguard: image: lscr.io/linuxserver/wireguard:latest container_name: wireguard hostname: wireguard cap_add: - NET_ADMIN environment: - PUID=${PUID} - PGID=${PGID} - TZ=${TZ} - PEERS=laptop,phone - SERVERURL=wireguard.example.com - SERVERPORT=${WIREGUARD_PORT} - INTERNAL_SUBNET=10.0.2.0/24 - PEERDNS=10.0.2.1 - ALLOWEDIPS=0.0.0.0/0 - LOG_CONFS=false healthcheck: test: ping -c 1 1.1.1.1 || exit 1 interval: 2s start_period: 10s start_interval: 2s timeout: 5s retries: 3 volumes: - ${CONFIG_DIR}/wireguard:/config - ${CONFIG_DIR}/wireguard_startup:/custom-cont-init.d:ro ports: - 80:80 - 443:443 - ${WIREGUARD_PORT}:${WIREGUARD_PORT}/udp - ${JELLYFIN_WEBUI_HTTP_PORT}:${JELLYFIN_WEBUI_HTTP_PORT} # direct access for Jellyfin mobile apps sysctls: - net.ipv4.conf.all.src_valid_mark=1 networks: - default - internal restart: always labels: - traefik.enable=true ## PiHole - traefik.http.routers.pihole.entrypoints=websecure - traefik.http.routers.pihole.rule=Host(`pihole.jmartins.dev`) - traefik.http.routers.pihole.service=pihole - traefik.http.services.pihole.loadbalancer.server.scheme=http - traefik.http.services.pihole.loadbalancer.server.port=${PIHOLE_WEBUI_PORT} ## Jellyfin - traefik.http.routers.jellyfin.entrypoints=websecure - traefik.http.routers.jellyfin.rule=Host(`jellyfin.jmartins.dev`) - traefik.http.routers.jellyfin.tls.certresolver=letsencrypt - traefik.http.routers.jellyfin.service=jellyfin - traefik.http.services.jellyfin.loadbalancer.server.scheme=http - traefik.http.services.jellyfin.loadbalancer.server.port=${JELLYFIN_WEBUI_HTTP_PORT} pihole: container_name: pihole image: pihole/pihole:latest network_mode: service:wireguard depends_on: wireguard: condition: service_healthy healthcheck: test: ping -c 1 google.com || exit 1 interval: 2s start_period: 10s start_interval: 2s timeout: 5s retries: 3 environment: TZ: ${TZ} FTLCONF_webserver_port: ${PIHOLE_WEBUI_PORT} FTLCONF_webserver_api_password: ${PIHOLE_PASSWORD} FTLCONF_dns_upstreams: \u0026#39;1.1.1.1\u0026#39; FTLCONF_dns_dnssec: true FTLCONF_dns_revServers: \u0026#39;true,192.168.1.0/24,192.168.1.1,lan\u0026#39; FTLCONF_misc_dnsmasq_lines: \u0026#34;address=/jmartins.dev/10.0.2.1\u0026#34; cap_add: - NET_ADMIN volumes: - ${CONFIG_DIR}/pihole/etc-pihole/:/etc/pihole/ - ${CONFIG_DIR}/pihole/etc-dnsmasq.d/:/etc/dnsmasq.d/ restart: always proxy: image: tecnativa/docker-socket-proxy container_name: proxy environment: - CONTAINERS=1 - SERVICES=1 - NETWORKS=1 - TASKS=1 - IMAGES=1 volumes: - /var/run/docker.sock:/var/run/docker.sock:ro networks: - internal restart: always traefik: image: traefik container_name: traefik network_mode: service:wireguard volumes: - /etc/localtime:/etc/localtime:ro - ${CONFIG_DIR}/traefik/letsencrypt:/letsencrypt command: - --api.dashboard=true - --certificatesresolvers.letsencrypt.acme.dnschallenge=true - --certificatesresolvers.letsencrypt.acme.dnschallenge.provider=cloudflare - --certificatesresolvers.letsencrypt.acme.email=me@example.com - --certificatesresolvers.letsencrypt.acme.storage=/letsencrypt/acme.json - --entrypoints.web.address=:80 - --entrypoints.websecure.address=:443 - --entrypoints.web.http.redirections.entryPoint.to=websecure - --entrypoints.web.http.redirections.entryPoint.scheme=https - --providers.docker=true - --providers.docker.exposedbydefault=false - --providers.docker.endpoint=tcp://proxy:2375 - --entrypoints.websecure.http.tls=true - --entrypoints.websecure.http.tls.certResolver=letsencrypt - --entrypoints.websecure.http.tls.domains[0].main=jmartins.dev - --entrypoints.websecure.http.tls.domains[0].sans=*.jmartins.dev - --log.level=INFO environment: - CF_DNS_API_TOKEN=[REDACTED] depends_on: pihole: condition: service_healthy proxy: condition: service_started restart: always jellyfin: image: lscr.io/linuxserver/jellyfin container_name: jellyfin network_mode: service:wireguard environment: - PUID=${PUID} - PGID=${PGID} - TZ=${TZ} - JELLYFIN_PublishedServerUrl=jellyfin.jmartins.dev volumes: - ${CONFIG_DIR}/jellyfin:/config - ${MOVIE_BACKUPS_DIR}:/data/movie_backups devices: - /dev/dri:/dev/dri depends_on: pihole: condition: service_healthy restart: always ntfy: image: binwiederhier/ntfy container_name: ntfy command: - serve environment: - TZ=${TZ} - NTFY_BASE_URL=https://ntfy.jmartins.dev - NTFY_AUTH_DEFAULT_ACCESS=deny-all - NTFY_BEHIND_PROXY=true - NTFY_ENABLE_LOGIN=true user: ${PUID}:${PGID} volumes: - ${CONFIG_DIR}/ntfy_cache:/var/lib/ntfy - ${CONFIG_DIR}/ntfy_config:/etc/ntfy healthcheck: test: [\u0026#34;CMD-SHELL\u0026#34;, \u0026#34;wget -q --tries=1 http://localhost:80/v1/health -O - | grep -Eo \u0026#39;\\\u0026#34;healthy\\\u0026#34;\\\\s*:\\\\s*true\u0026#39; || exit 1\u0026#34;] interval: 60s timeout: 10s retries: 3 start_period: 40s labels: - traefik.enable=true - traefik.http.routers.ntfy.entrypoints=websecure - traefik.http.routers.ntfy.rule=Host(`ntfy.jmartins.dev`) - traefik.http.routers.ntfy.tls.certresolver=letsencrypt - traefik.http.routers.ntfy.service=ntfy - traefik.http.services.ntfy.loadbalancer.server.scheme=http - traefik.http.services.ntfy.loadbalancer.server.port=80 restart: always networks: default: name: docker-stack-network internal: name: traefik-internal internal: true The corresponding .env file:\n### .ENV FILE ### # ======== user ======== PUID=1000 PGID=1000 TZ=Australia/Sydney # ======== directories ======== CONFIG_DIR=/home/jmartins/docker-stack/configs MOVIE_BACKUPS_DIR=/mnt/media/movie_backups # ======== network ======== WIREGUARD_PORT=51820 # ======== service ports ======== PIHOLE_WEBUI_PORT=9000 PIHOLE_PASSWORD=your_pihole_password JELLYFIN_WEBUI_HTTP_PORT=8096 You\u0026rsquo;ll notice that Jellyfin\u0026rsquo;s HTTP port is exposed directly on the WireGuard container in addition to being routed through Traefik on 443. This is for Jellyfin\u0026rsquo;s mobile apps, which can connect directly over the tunnel using the raw HTTP port without going through the reverse proxy.\nTwo Docker networks are at play here. The default network is where most services live. The internal network exists solely for Traefik to communicate with the Docker Socket Proxy — its internal: true flag prevents any container on it from accessing the internet, limiting the blast radius if the proxy were compromised. The WireGuard container connects to both networks, bridging them. Docker handles IP assignment and DNS resolution for container hostnames automatically, so services can reference each other by name (e.g. Traefik reaches the proxy at tcp://proxy:2375) without needing hardcoded addresses.\nConnecting from the Outside # With the stack running, it\u0026rsquo;s time to connect a device. The Linuxserver.io WireGuard image generates peer configurations in /config/peer_laptop/, /config/peer_phone/, and so on. Each folder contains a peer_\u0026lt;name\u0026gt;.conf file and a QR code PNG that you can scan with the WireGuard mobile app.\nA generated peer configuration looks something like this:\n[Interface] Address = 10.0.2.2/32 PrivateKey = [REDACTED] ListenPort = 51820 DNS = 10.0.2.1 [Peer] PublicKey = [REDACTED] PresharedKey = [REDACTED] Endpoint = wireguard.example.com:51820 AllowedIPs = 0.0.0.0/0 The key fields: DNS = 10.0.2.1 points to PiHole on the tunnel, and AllowedIPs = 0.0.0.0/0 routes all traffic through the VPN. Import this on your device, connect, and let\u0026rsquo;s verify everything works.\nFrom the connected laptop:\n$ curl https://am.i.mullvad.net/connected You are connected to Mullvad (server au14-wireguard). Your IP address is 89.44.10.183 $ dig +short jellyfin.jmartins.dev @10.0.2.1 10.0.2.1 $ curl -sI https://jellyfin.jmartins.dev | head -5 HTTP/2 200 content-type: text/html; charset=utf-8 x-response-time-ms: 12 server: Kestrel date: Sun, 23 Feb 2026 10:00:00 GMT The first command confirms our traffic exits through Mullvad — same as in the previous article, but now from a remote device rather than from inside the container. The second shows that PiHole resolves jellyfin.jmartins.dev to 10.0.2.1, the WireGuard tunnel address. The third confirms Traefik is routing the HTTPS request to Jellyfin and serving a valid certificate.\nWe can also verify the WireGuard container is running both interfaces by execing into it:\nroot@wireguard:/# wg show interface: wg0 public key: [REDACTED] private key: (hidden) listening port: 51820 fwmark: 0xca6c peer: [REDACTED] endpoint: 203.0.113.45:51820 allowed ips: 10.0.2.2/32 latest handshake: 42 seconds ago transfer: 156.78 MiB received, 1.23 GiB sent interface: wg1 public key: [REDACTED] private key: (hidden) listening port: 41983 fwmark: 0xca6c peer: [REDACTED] endpoint: 89.44.10.178:51820 allowed ips: 0.0.0.0/0 latest handshake: 1 minute, 12 seconds ago transfer: 2.45 GiB received, 892.34 MiB sent wg0 is the server interface with our laptop connected as a peer. wg1 is the Mullvad client tunnel. Both are up, both are transferring data, and the fwmark on both matches our kill switch exemption value of 0xca6c.\nConclusion # What started in the previous article as a single VPN client container with a kill switch has evolved into a proper remote access stack. The WireGuard container now sits at the centre of the network, simultaneously serving as a VPN client to Mullvad and a VPN server for remote devices. PiHole provides ad-blocking DNS for every connected device, with a dnsmasq trick that routes service subdomains back through the WireGuard tunnel to Traefik. Traefik handles HTTPS termination with a Let\u0026rsquo;s Encrypt wildcard certificate obtained via DNS challenge, routing requests to the correct service without any of them being directly exposed to the internet.\nThe stack fails closed — if the Mullvad connection drops, the kill switch blocks all outbound traffic for services sharing the WireGuard network. If the WireGuard configuration fails to parse, the iptables rules from the startup script are already in place. Services that need to maintain outbound connectivity regardless of VPN state, such as notification and monitoring services, can be placed on their own Docker network to bypass the kill switch while remaining accessible to remote clients through Traefik.\nEvery device connecting through the VPN gets ad blocking, encrypted DNS, and access to self-hosted services, all through a single WireGuard connection. Adding a new service is a matter of defining the container with network_mode: service:wireguard (or its own network, depending on the use case), adding Traefik labels for routing, and exposing the port on the WireGuard container. PiHole\u0026rsquo;s wildcard dnsmasq rule handles DNS automatically. No firewall changes, no certificate requests, no client-side configuration.\nDocker Compose network_mode docs\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLinuxserver.io WireGuard guide\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLinuxserver.io Custom Scripts\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRFC 1918 — Address Allocation for Private Internets\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPi-hole — Network-wide ad blocking\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTraefik — Cloud Native Application Proxy\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTecnativa Docker Socket Proxy\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLet\u0026rsquo;s Encrypt DNS-01 Challenge\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nntfy — Push notifications\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"23 February 2026","externalUrl":null,"permalink":"/posts/wireguard-docker-vpn-server/","section":"","summary":"","title":"Running WireGuard as Client and Server in Docker with PiHole and Traefik","type":"posts"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/tags/traefik/","section":"Tags","summary":"","title":"Traefik","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/tags/vpn/","section":"Tags","summary":"","title":"VPN","type":"tags"},{"content":"","date":"23 February 2026","externalUrl":null,"permalink":"/tags/wireguard/","section":"Tags","summary":"","title":"WireGuard","type":"tags"},{"content":"TL;DR If you don’t feel like reading my ramblings, the Github repository with the final state of the docker-compose.yaml file, as well as the necessary configuration files can be found here.\nIf you\u0026rsquo;re anything like me, you like to tinker with everything technology-related. You\u0026rsquo;re also weary of fingerprinting and tracking of your online habits, and would like to set up a way of routing your self-hosted services like PiHole through a VPN, while keeping everything easily configurable and transferable. If you\u0026rsquo;re nodding your head right now, you\u0026rsquo;ve come to the right place.\nContainers # Containers, in this context, are used as a reference to any containerization platform that performs OS-level virtualization through multiple userspace container instances. The benefit of using containers is that we are able to encapsulate the software and its dependencies into a single package that can then be deployed in different environments. Configuration of these \u0026ldquo;containers\u0026rdquo; should be done through files, making a deployment replicable and transferable across different environments.\nWireGuard # Like the older OpenVPN protocol, WireGuard is a protocol and software implementation for establishing virtual private networks (VPNs). It excels in its lower overhead relative to older protocols, its high performance and its easy configuration. The protocol was released in the Linux 5.6 kernel, which we will be making use of its kernel modules inside a container by exposing /lib/modules. The standard software implementation of the WireGuard protocol is also called WireGuard, which may cause some confusion, but all we need to know is that it the application that allows us to interact with the wireguard kernel module. Most of the time we\u0026rsquo;ll be using wg-quick, the utility to establish and stop WireGuard VPN connections.\nLinuxserver.io # Linuxserver.io is a community and community-maintained list of docker container images who follow a unified best-practices approach to their container images, while maintaining small container sizes and some helpful added-on functionality that we will be making use of for implementing the kill switch. They maintain a WireGuard (userspace utilities) image that we will use as the outbound VPN container through which all other containers will connect to the internet. Using Linuxserver.io\u0026rsquo;s /custom-cont-init.d folder we can add a startup script that will be run before the VPN connection is made. The GitHub image repository can be found here.\nVPN # It\u0026rsquo;s now time to set up our own VPN client with a correctly configured kill switch and add some other images to use it. The first step is to get your own VPN provider, I have used Mullvad VPN for a few years as my VPN provider of choice and have never had any issues with them. They run a no-log VPN service, their clients are open source, and they helped fund WireGuard\u0026rsquo;s development, so it\u0026rsquo;s a pretty easy choice for me. Most importantly, in this case, is that you can generate a WireGuard client configuration that you can then use as the configuration for your Linuxserver.io WireGuard container.\nPutting it all together # OK, we\u0026rsquo;re ready to create our stack. Following Linuxserver.io WireGuard container\u0026rsquo;s instructions, we can generate our first docker-compose.yaml.\n### DOCKER-COMPOSE.YAML FILE ### services: wireguard: image: lscr.io/linuxserver/wireguard:latest container_name: wireguard hostname: wireguard cap_add: - NET_ADMIN - SYS_MODULE environment: - PUID=${PUID} - PGID=${PGID} - TZ=${TZ} volumes: - ${CONFIG_DIR}/wireguard:/config - ${CONFIG_DIR}/wireguard_startup:/custom-cont-init.d:ro - /lib/modules:/lib/modules sysctls: - net.ipv4.conf.all.src_valid_mark=1 restart: unless-stopped The variables in the docker-compose.yaml file are saved in the .env file, which get automatically used by docker-compose:\n### .ENV FILE ### # ======== user ======== PUID=1000 PGID=1000 TZ=Australia/Sydney # ======== directories ======== CONFIG_DIR=/home/jmartins/wireguard-stack/configs In order to use WireGuard we need to drop a WireGuard configuration file in the container\u0026rsquo;s /config folder that we map to our ${CONFIG_DIR}/wireguard folder. Generating this file on Mullvad\u0026rsquo;s website is fairly easy, and in this case we\u0026rsquo;re only using an IPv4-only configuration since we want to manually control the ports available from outside the host. The generated configuration should look something like this:\n[Interface] PrivateKey = [REDACTED] Address = 10.64.114.74/32 DNS = 10.64.0.1 [Peer] PublicKey = a6oniBujlUXqOmv5Hst0v8xCqidy7O4JcN8Q6YRM5Hk= AllowedIPs = 0.0.0.0/0 Endpoint = 89.44.10.178:51820 If we run the docker-compose up -d command, the default network will now be created, along with the docker container. We can test the spun up container by executing docker exec -it wireguard /bin/bash to get access to a bash session inside the running container. Here, we can run regular network tests like curling Mullvad\u0026rsquo;s connection check URLand pinging IP addresses:\nroot@wireguard:/# curl https://am.i.mullvad.net/connected You are connected to Mullvad (server au14-wireguard). Your IP address is 89.44.10.183 root@wireguard:/# ping 1.1.1.1 PING 1.1.1.1 (1.1.1.1) 56(84) bytes of data. 64 bytes from 1.1.1.1: icmp_seq=1 ttl=58 time=3.84 ms 64 bytes from 1.1.1.1: icmp_seq=2 ttl=58 time=3.82 ms 64 bytes from 1.1.1.1: icmp_seq=3 ttl=58 time=3.29 ms ^C --- 1.1.1.1 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2004ms rtt min/avg/max/mdev = 3.293/3.651/3.838/0.253 ms You are now ready to start routing connections through this container by setting their network mode as \u0026quot;service:wireguard\u0026quot;. But before we get to that, lets take care of the network kill switch, which will prevent requests to flow outside the network if for some reason the VPN connection is dropped.\nWireGuard Kill Switch # Generating the wg0.conf file on Mullvad\u0026rsquo;s website allows us to add a kill switch to the wireguard configuration. These are nothing but shell commands that run PostUp and PreDown, up and down referring to wg-quick\u0026rsquo;s verbs for starting and stopping the VPN client connection.\nCreating a WireGuard configuration with a kill switch yields the following file:\n[Interface] PrivateKey = [REDACTED] Address = 10.64.23.84/32,fc00:bbbb:bbbb:bb01::1:1753/128 DNS = 10.64.0.1 PostUp = iptables -I OUTPUT ! -o %i -m mark ! --mark $(wg show %i fwmark) -m addrtype ! --dst-type LOCAL -j REJECT \u0026amp;\u0026amp; ip6tables -I OUTPUT ! -o %i -m mark ! --mark $(wg show %i fwmark) -m addrtype ! --dst-type LOCAL -j REJECT PreDown = iptables -D OUTPUT ! -o %i -m mark ! --mark $(wg show %i fwmark) -m addrtype ! --dst-type LOCAL -j REJECT \u0026amp;\u0026amp; ip6tables -D OUTPUT ! -o %i -m mark ! --mark $(wg show %i fwmark) -m addrtype ! --dst-type LOCAL -j REJECT [Peer] PublicKey = pu22RCPeJCeiDIE7a1XtWvmv3BdgPp8ugF6AyntW8xU= AllowedIPs = 0.0.0.0/0,::0/0 Endpoint = 89.44.10.114:51820 The kill switch commands consist of iptables rules blocking all outbound traffic in the container and allowing only traffic to flow through the wireguard network adapter.\nUsing the downloaded configuration file, we can see that stopping the connection will remove the iptables rules, allowing traffic to flow out:\nroot@wireguard:/# curl https://am.i.mullvad.net/connected You are connected to Mullvad (server au14-wireguard). Your IP address is 89.44.10.183 root@wireguard:/# ping 1.1.1.1 PING 1.1.1.1 (1.1.1.1) 56(84) bytes of data. 64 bytes from 1.1.1.1: icmp_seq=1 ttl=58 time=3.28 ms 64 bytes from 1.1.1.1: icmp_seq=2 ttl=58 time=11.6 ms 64 bytes from 1.1.1.1: icmp_seq=3 ttl=58 time=4.08 ms ^C --- 1.1.1.1 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2004ms rtt min/avg/max/mdev = 3.278/6.304/11.559/3.730 ms root@wireguard:/# wg-quick down wg0 Warning: `/config/wg0.conf\u0026#39; is world accessible [#] iptables -D OUTPUT ! -o wg0 -m mark ! --mark $(wg show wg0 fwmark) -m addrtype ! --dst-type LOCAL -j REJECT \u0026amp;\u0026amp; ip6tables -D OUTPUT ! -o wg0 -m mark ! --mark $(wg show wg0 fwmark) -m addrtype ! --dst-type LOCAL -j REJECT [#] ip -4 rule delete table 51820 [#] ip -4 rule delete table main suppress_prefixlength 0 [#] ip -6 rule delete table 51820 [#] ip -6 rule delete table main suppress_prefixlength 0 [#] ip link delete dev wg0 [#] resolvconf -d wg0 -f [#] iptables-restore -n [#] ip6tables-restore -n root@wireguard:/# curl https://am.i.mullvad.net/connected You are not connected to Mullvad. Your IP address is 161.8.193.91 root@wireguard:/# ping 1.1.1.1 PING 1.1.1.1 (1.1.1.1) 56(84) bytes of data. 64 bytes from 1.1.1.1: icmp_seq=1 ttl=57 time=2.66 ms 64 bytes from 1.1.1.1: icmp_seq=2 ttl=57 time=3.87 ms 64 bytes from 1.1.1.1: icmp_seq=3 ttl=57 time=3.30 ms ^C --- 1.1.1.1 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2003ms rtt min/avg/max/mdev = 2.657/3.276/3.872/0.496 ms Note on DNS resolution vs outbound traffic # There is a possibility that you may confuse lack of DNS resolution with blocked outbound traffic if your IP block overlaps with your container network subnet. This is unlikely to happen and that would mean that DNS resolution was blocked regardless of iptables rules, but it also means that you may be mistaken on the state of your container network rules. This could, for example, allow through traffic that does not depend on DNS, like say a peer-to-peer connection.\nThe solution is then to remove the PreDown directive, leading to an outbound blocked connection state whenever the connection is brought down.\nroot@wireguard:/# wg-quick down wg0 Warning: `/config/wg0.conf\u0026#39; is world accessible [#] ip -4 rule delete table 51820 [#] ip -4 rule delete table main suppress_prefixlength 0 [#] ip -6 rule delete table 51820 [#] ip -6 rule delete table main suppress_prefixlength 0 [#] ip link delete dev wg0 [#] resolvconf -d wg0 -f [#] iptables-restore -n [#] ip6tables-restore -n root@wireguard:/# curl https://am.i.mullvad.net/connected curl: (6) Could not resolve host: am.i.mullvad.net root@wireguard:/# ping 1.1.1.1 PING 1.1.1.1 (1.1.1.1) 56(84) bytes of data. From 10.0.0.2 icmp_seq=1 Destination Port Unreachable ping: sendmsg: Operation not permitted From 10.0.0.2 icmp_seq=2 Destination Port Unreachable ping: sendmsg: Operation not permitted From 10.0.0.2 icmp_seq=3 Destination Port Unreachable ping: sendmsg: Operation not permitted ^C --- 1.1.1.1 ping statistics --- 3 packets transmitted, 0 received, +3 errors, 100% packet loss, time 2032ms A less curious/paranoid person would at this point be happy with the kill switch functionality. However, I am neither of those. Since the kill switch depends on a successful parsing of the wireguard configuration file, an issue presents itself with the way the container handles failure. In short, if there are any issues parsing the configuration file, the container will not connect to the VPN server and continue allowing outbound network calls to flow through. Silently.\nIn a borrowed term from mechanical engineering into application/network security, this is a case of fail open, where a failure on startup will lead to a permissive state. If we purposely create this error, we can see the issue.\n[Interface] PrivateKey = [REDACTED] Address # This malformed configuration leads to an error DNS = 10.64.0.1 PostUp = iptables -I OUTPUT ! -o %i -m mark ! --mark $(wg show %i fwmark) -m addrtype ! --dst-type LOCAL -j REJECT \u0026amp;\u0026amp; ip6tables -I OUTPUT ! -o %i -m mark ! --mark $(wg show %i fwmark) -m addrtype ! --dst-type LOCAL -j REJECT [Peer] PublicKey = a6oniBujlUXqOmv5Hst0v8xCqidy7O4JcN8Q6YRM5Hk= AllowedIPs = 0.0.0.0/0 Endpoint = 89.44.10.178:51820 Docker log of the container startup:\n... wireguard | Warning: `/config/wg0.conf\u0026#39; is world accessible wireguard | [#] ip link add wg0 type wireguard wireguard | [#] wg setconf wg0 /dev/fd/63 wireguard | [#] ip -4 address add Address dev wg0 wireguard | Error: inet prefix is expected rather than \u0026#34;Address\u0026#34;. wireguard | [#] ip link delete dev wg0 ... We can then see that we are able to make outbound requests from the container, as if we were connected and blocking requests outside the VPN tunnel:\nroot@wireguard:/# curl https://am.i.mullvad.net/connected You are not connected to Mullvad. Your IP address is X.X.X.X If we would like to make this a fail close system, a solution is to decouple the iptables outbound rules setting from the WireGuard execution. Making use of the Linuxserver container addons we can add a script that will be run on container startup and will block outbound connections the same way as PostUp does.\n#!/bin/bash echo \u0026#34;**** IPTABLES BLOCK ****\u0026#34; iptables -I OUTPUT ! -o wg0 -m mark ! --mark 0xca6c -m addrtype ! --dst-type LOCAL -j REJECT ip6tables -I OUTPUT ! -o wg0 -m mark ! --mark 0xca6c -m addrtype ! --dst-type LOCAL -j REJECT wg-quick automatically adds a mark on all encrypted packets it sends1, with the value of this mark being the port it is configured to connect through. If you use the default 51820 port, then the iptables rules matching 0xca6c will work, as this is the hexadecimal representation of decimal 51820. If you use any other port, then you must change this script to the correct hexadecimal value, and if you forget to do this, no traffic will be allowed to flow outbound from the container. Again, failing closed.\nroot@wireguard:/# curl https://am.i.mullvad.net/connected curl: (6) Could not resolve host: am.i.mullvad.net root@wireguard:/# ping 1.1.1.1 PING 1.1.1.1 (1.1.1.1) 56(84) bytes of data. From 10.0.0.2 icmp_seq=1 Destination Port Unreachable ping: sendmsg: Operation not permitted From 10.0.0.2 icmp_seq=2 Destination Port Unreachable ping: sendmsg: Operation not permitted From 10.0.0.2 icmp_seq=3 Destination Port Unreachable ping: sendmsg: Operation not permitted ^C --- 1.1.1.1 ping statistics --- 3 packets transmitted, 0 received, +3 errors, 100% packet loss, time 2032ms Note: In the default Mullvad wireguard configuration, the packet mark is obtained by running wg show %i fwmark, where %i is the wireguard interface name as expanded in wg-quick2. This won\u0026rsquo;t work here, as the startup script will run before the wireguard interface is created, and thus we instead hardcode the mark to 0xca6c instead, ensuring that, regardless of the wireguard connection state, only packets marked with 0xca6c will be allowed to egress.\nVPN consumers # Now that we have the container ready and we\u0026rsquo;re correctly stopping requests we don\u0026rsquo;t want to proceed, we can configure other containerised images that we will route through the VPN container exclusively. This will both route all their network requests through the WireGuard container as well as make them subject to its outbound network rules containing the kill switch. For this, we\u0026rsquo;ll use docker-compose\u0026rsquo;s network_mode3 option, as suggested in this Linuxserver.io article4, allowing a container to make use of a different container\u0026rsquo;s network stack, in this case the WireGuard container.\nAs an example consumer application, we\u0026rsquo;re using here the thespeedtest-tracker, which will periodically, every ten minutes, run a speed test in its network interface and store the result. Since we\u0026rsquo;re forcing the container to use the wireguard container\u0026rsquo;s network stack, its iptables rules will apply and all traffic will flow through the wireguard interface.\n### DOCKER-COMPOSE.YAML FILE ### services: wireguard: image: lscr.io/linuxserver/wireguard:latest container_name: wireguard hostname: wireguard cap_add: - NET_ADMIN - SYS_MODULE environment: - PUID=${PUID} - PGID=${PGID} - TZ=${TZ} volumes: - ${CONFIG_DIR}/wireguard:/config - ${CONFIG_DIR}/wireguard_startup:/custom-cont-init.d:ro - /lib/modules:/lib/modules ports: - 8080:80 sysctls: - net.ipv4.conf.all.src_valid_mark=1 healthcheck: test: ping -c 1 1.1.1.1 || exit 1 interval: 2s start_period: 10s start_interval: 2s timeout: 5s retries: 3 restart: unless-stopped speedtest-tracker: image: lscr.io/linuxserver/speedtest-tracker:latest restart: unless-stopped container_name: speedtest-tracker network_mode: service:wireguard environment: - PUID=${PUID} - PGID=${PGID} - TZ=${TZ} - APP_KEY=${APP_KEY} - DB_CONNECTION=sqlite - SPEEDTEST_SCHEDULE=\u0026#34;*/10 * * * *\u0026#34; - DISPLAY_TIMEZONE=${TZ} volumes: - ${CONFIG_DIR}/speedtest-tracker:/config healthcheck: test: curl -fSs http://localhost/api/healthcheck | jq -r .message || exit 1 interval: 10s retries: 3 start_period: 30s timeout: 10s depends_on: wireguard: condition: service_healthy As you may notice, since the network is managed by the wireguard container, in order to expose the port that speedtest-tracker serves its web interface in, port 80, this port forward needs to be controlled on the wireguard ports directive instead. Here, we\u0026rsquo;ve decided to port forward that port to port 8080, which we\u0026rsquo;ll use to access the speedtest-tracker web UI.\nFinally, like described on the linuxserver.io guide4:\nBut it doesn\u0026rsquo;t end there. Even though the port is mapped, once the tunnel is up, it won\u0026rsquo;t respond to any requests coming from the host as it\u0026rsquo;s configured to send all outgoing connections through the tunnel.\nThis means we need to add the routing rules to allow the host to access the container\u0026rsquo;s web interface. In the article, this is done as part of the wg-quick configuration file, but since we are using a shell script to set the kill switch rules, we can add the routing rules there as well. The following lines will allow the host to access the speedtest-tracker web interface:\n#!/bin/bash set -e echo \u0026#34;**** Adding iptables rules ****\u0026#34; HOMENET=192.168.0.0/16 HOMENET2=10.0.0.0/8 HOMENET3=172.16.0.0/12 iptables -I OUTPUT -d $HOMENET -j ACCEPT iptables -A OUTPUT -d $HOMENET2 -j ACCEPT iptables -A OUTPUT -d $HOMENET3 -j ACCEPT # Kill switch iptables -A OUTPUT ! -o wg0 -m mark ! --mark 0xca6c -m addrtype ! --dst-type LOCAL -j REJECT ip6tables -I OUTPUT ! -o wg0 -m mark ! --mark 0xca6c -m addrtype ! --dst-type LOCAL -j REJECT echo \u0026#34;**** Successfully added iptables rules ****\u0026#34; Here, the various HOMENETs are the local network IP ranges usually used in home networks, as defined in RFC-19185. We add the iptables rules to allow traffic to flow out to the local network, including the web UI of the speedtest-tracker container, which is now accessible at http://localhost:8080.\nConclusion # In this article, we have seen how to set up a WireGuard VPN container that can be used as a kill switch for other containers. We have also seen how to set up a consumer container that uses the WireGuard container\u0026rsquo;s network stack and how to expose its web interface to the host. The kill switch is implemented using iptables rules that block all outbound traffic unless it is going through the WireGuard interface, and we have ensured that the system fails closed by decoupling the kill switch from the WireGuard connection setup.\nThis was never intended to be a series of articles, but since it took me so long to finally write it down, my local wireguard stack has evolved quite a bit, including running the WireGuard container as both a client and a server, allowing clients to connect to it over the internet, routing their traffic through the Mullvad VPN connection and giving access to the containers running on the host. Expect a follow-up article on this topic in the future.\nWg-quick Default Firewall Rules\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nwg-quick manpage\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDocker compose network_mode docs\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLinuxserver.io Wireguard guide\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRFC-1918\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"18 July 2025","externalUrl":null,"permalink":"/posts/wireguard-docker-killswitch/","section":"","summary":"","title":"Routing Docker Containers Through a WireGuard VPN Container with Kill Switch","type":"posts"},{"content":" Python-based static webpage # Previously, my web presence was composed of a static page built using staticjinja, an HTML5 UP template, and my own json parsing logic to build the different elements displayed. Back when I made it I had little experience with Javascript and no wish or need to update it continually as one does with a blog.1 It looked something like this:\nIt worked well as I didn\u0026rsquo;t have the need to update it often, but perhaps leading to it being usually very out-of-date as I changed projects and workplaces. Adding to this, the advent — or should I say return? — of blogging and developer diaries made me want to start my own blog. This would be where I share what I learn along the way, some tricks and tips on Python and development strategies that seem to work for me.\nJavascript static site generators # While Medium seems to be the platform of choice for these kinds of blogging, I\u0026rsquo;m a proponent of free open source software and of learning-by-doing, so this was a good opportunity to learn some more Javascript, React and Gatsby, an open source static site generator with good Markdown support.\nMaking it mine # I have always had an obsessive relationship with tinkering and making my digital experiences as close to my liking as possible. A good example is Dark Reader, an open source browser extension that \u0026ldquo;inverts brightness of web pages and aims to reduce eyestrain while you browse the web\u0026rdquo;, which I currently use for handling pages that don\u0026rsquo;t offer a native dark mode.\nThis website is based on Lumen, a Gatsby starter blog with a minimalistic and pleasing design. It does not support a dark mode out of the box, making its implementation an interesting learning experience. It also seems to be a request from people using the starter blog, so it\u0026rsquo;s a good opportunity to help out.\nAfter some research — figuring out the quality of developer blog posts has become a skill in itself — I had a few good ideas on how to do this. Ananya\u0026rsquo;s article2 on dev.to was a great starting point, especially since most other articles don\u0026rsquo;t suggest CSS custom properties (also called variables) for implementing dark mode.\nThe most common approach seems to be defining an alternative dark mode global \u0026lt;body\u0026gt; class and using Javascript before the body renders to edit its class to the dark mode explicitly. Since Lumen already makes use of Sass, I went with a mixed approach, keeping Sass variables in the different components and assigning to them the CSS custom properties.\nThis has its drawbacks too, as we can\u0026rsquo;t use Sass color module functions, but the very limited number of colors and page types of the template allows for pre-setting all the colors we will use. It also has its benefits, since we can directly use CSS attribute selectors and HMTL5\u0026rsquo;s data-* global attributes. We can use an attribute selector dependent on our data-* attribute, changing the color variables depending on which theme data attribute we select.\nMedia queries and setting a mode # Media Queries allow authors to test and query values or features of the user agent or display device, independent of the document being rendered. They are used in the CSS @media rule to conditionally apply styles to a document, and in various other contexts and languages, such as HTML and JavaScript.\n— Media Queries Level 5 specification\nWe\u0026rsquo;re connecting another recent feature of CSS formatting, media queries and, more specifically, the prefers-color-scheme media query that informs the browser of the client\u0026rsquo;s OS dark/light mode preference. With this I can infer the reader\u0026rsquo;s preference for a dark or light mode and use that preference to style the page accordingly.\nconst mql = window.matchMedia(\u0026#39;(prefers-color-scheme: dark)\u0026#39;); const hasMediaQueryPreference = typeof mql.matches === \u0026#39;boolean\u0026#39;; if (hasMediaQueryPreference \u0026amp;\u0026amp; mql.matches === true) { document.documentElement.dataset.theme = \u0026#39;dark\u0026#39;; } else { document.documentElement.dataset.theme = \u0026#39;light\u0026#39; } mql should hold a boolean, indicating if the user\u0026rsquo;s OS color preference is dark mode. Like I mentioned before, I\u0026rsquo;m using data-* attributes, which means that we can use the dataset object of documentElement. Subsequently, I set my alternative CSS selector to use these data-* attribute:\n// Colors, using css variables :root { // Based on One Light: https://github.com/atom/one-light-syntax/blob/master/styles/colors.less --bg-color: rgb(231, 230, 223); --base: rgb(11, 23, 82); --primary: rgb(134, 69, 28); --secondary: rgba(11, 23, 82, 70%); --gray: hsl(230, 23%, 23%); --gray-border: hsl(230, 77%, 13%); } [data-theme=\u0026#34;dark\u0026#34;] { // Based on One Dark: https://github.com/atom/atom/blob/master/packages/one-dark-syntax/styles/colors.less --bg-color: hsl(220, 13%, 18%); --base: hsl(219, 14%, 71%); // mono-1 --primary: hsl( 29, 54%, 61%); // orange-1 --secondary: hsl(220, 9%, 55%); // mono-2 --gray-border: hsl(220, 10%, 40%); // mono-3 --gray: hsl(0, 0%, 100%); // white } As discussed before, depending on the root data attribute, the page will either display the default CSS colors, or the dark mode colors.\nFlash of Unstyled Content # The flash of unstyled content (FOUC) is a fairly annoying consequence of sequential DOM building, where a browser will display HTML without having fully loaded its CSS. It is especially noticeable on dark/light mode pages when the default flashing page mode is different from the expected OS color mode, and getting around this issue is fairly easy with Gatsby.\nIn order to avoid this we\u0026rsquo;ll load and set the preferred media mode (dark or light) using Gatsby\u0026rsquo;s setPreBodyComponents function in its server side render options. When using this specific starter blog, editing the gatsby/on-render-body.js file is where these changes should be placed, since this file is referenced by the Gatsby server side rendering file, gastby-ssr.js.\nconst applyDarkModeFunc = ` (function() { const mode = localStorage.getItem(\u0026#39;theme\u0026#39;); if (mode !== null \u0026amp;\u0026amp; [\u0026#39;light\u0026#39;, \u0026#39;dark\u0026#39;].includes(mode)) { document.documentElement.dataset.theme = mode; return; } const mql = window.matchMedia(\u0026#39;(prefers-color-scheme: dark)\u0026#39;); const hasMediaQueryPreference = typeof mql.matches === \u0026#39;boolean\u0026#39;; if (hasMediaQueryPreference \u0026amp;\u0026amp; mql.matches === true) { document.documentElement.dataset.theme = \u0026#39;dark\u0026#39;; } else { document.documentElement.dataset.theme = \u0026#39;light\u0026#39; } })(); `; const onRenderBody = ({ setPreBodyComponents }) =\u0026gt; { setPreBodyComponents([ React.createElement(\u0026#39;script\u0026#39;, { dangerouslySetInnerHTML: { __html: applyDarkModeFunc, }, }), ]); }; As you can see, before drawing the page body we check for a stored preference in localStorage and return if there is one. If not, we check the OS\u0026rsquo;s preferred color scheme and set te page theme accordingly.\nReact Hooks and Toggler # Finally, we need to create a button to change between dark and light mode. For this we\u0026rsquo;re using React\u0026rsquo;s functional programming hooks, making use of useState and useEffect. The first will be used to create a theme state variable and setter. Using this theme variable, we\u0026rsquo;ll register it as a dependency of the useEffect-triggered function, which will be run whenever the theme changes.\nWhenever the button is pressed and the toggleTheme function runs, the theme variable is changed to the appropriate new value by triggering our anonymous function:\nimport React, { useState, useEffect } from \u0026#39;react\u0026#39;; import styles from \u0026#39;./DarkModeToggler.module.scss\u0026#39;; function ThemeToggler() { const initTheme = document.documentElement.dataset.theme; const [theme, setTheme] = useState(initTheme); useEffect(() =\u0026gt; { localStorage.setItem(\u0026#39;theme\u0026#39;, theme); document.documentElement.dataset.theme = theme; }, [theme]); function toggleTheme() { const newTheme = theme === \u0026#39;dark\u0026#39; ? \u0026#39;light\u0026#39; : \u0026#39;dark\u0026#39;; setTheme(newTheme); } return ( \u0026lt;div className={styles[\u0026#39;toggler\u0026#39;]}\u0026gt; \u0026lt;label\u0026gt; \u0026lt;input type=\u0026#34;checkbox\u0026#34; onClick={() =\u0026gt; toggleTheme()} hidden={true} /\u0026gt;{`${theme} mode`} \u0026lt;/label\u0026gt; \u0026lt;/div\u0026gt; ); } export default ThemeToggler; Finishing steps # With all of this in place we are pretty much done! Small details like the medium-style zoom provided by gatsby-remark-images-medium-zoom can be configured in the gatsby-config.js page. I\u0026rsquo;d recommend using a CSS attribute (as a string) as the backgroung option, which will make it work with the dark/light mode settings too!\nThis is all I needed to do to make this work, feel free to replicate and modify it if you\u0026rsquo;re looking for the same functionality. I don\u0026rsquo;t have any comments section on this blog (on purpose), so feel free to open an issue on this page\u0026rsquo;s repo or message me through LinkedIn.\nPost Script 1 # While building and deploying I found a issue with SSR and the React component I built. The issue is that the browser DOM methods aren\u0026rsquo;t available when building on server side, but following Gatsby\u0026rsquo;s documentation we can work around the problem. To not have to add another dependency I decided to use React component lazy loading and check for the availability of the DOM methods, following the workaround 4 in the linked page.\nThese changes were introduced in the component that loads that offending component, Sidebar.js, and building now works correctly, as well as in development mode.\nThe code is still available under the gh-page-staticjinja-based branch backing this repo.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nArticle by Ananya Neogi.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"31 May 2021","externalUrl":null,"permalink":"/posts/dark-mode/","section":"","summary":"","title":"Gatsby, Lumen and Dark Mode","type":"posts"},{"content":"\nMostly little things that I learn here and there, updates seldom and mostly in bursts. ","externalUrl":null,"permalink":"/about/","section":"Learn Something New","summary":"","title":"","type":"page"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]